{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent\n",
    "from typing import Generator, Dict, Any\n",
    "import itertools\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import os\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import pandas as pd\n",
    "import ast\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "boto3.setup_default_session(profile_name=\"r2\")\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "image_bucket = \"salad-benchmark-public-assets\"\n",
    "bucket_domain = \"https://salad-benchmark-assets.download\"\n",
    "\n",
    "salad_green = \"#53a626\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_all_file_urls(bucket: str, prefix: str) -> Generator[str, None, None]:\n",
    "    paginator = s3_client.get_paginator(\"list_objects_v2\")\n",
    "    page_iterator = paginator.paginate(Bucket=bucket, Prefix=prefix)\n",
    "    for page in page_iterator:\n",
    "        for content in page[\"Contents\"]:\n",
    "            yield f\"{bucket_domain}/{content['Key']}\"\n",
    "\n",
    "\n",
    "def create_session(max_retries=3, backoff_factor=0.3):\n",
    "    \"\"\"\n",
    "    Create a requests session with retry strategy.\n",
    "    :param max_retries: Maximum number of retries for each request.\n",
    "    :param backoff_factor: A backoff factor to apply between attempts.\n",
    "    :param status_forcelist: A set of HTTP status codes that we should force a retry on.\n",
    "    :return: A requests session object.\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=max_retries,\n",
    "        read=max_retries,\n",
    "        connect=max_retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=[i for i in range(500, 600)],\n",
    "        respect_retry_after_header=True,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session\n",
    "\n",
    "\n",
    "# Usage\n",
    "session = create_session()\n",
    "\n",
    "num_requests = 0\n",
    "num_responses = 0\n",
    "\n",
    "\n",
    "def http_worker(request_params: Dict[str, Any]) -> requests.Response:\n",
    "    \"\"\"Function to make an HTTP request\"\"\"\n",
    "    global num_requests, num_responses\n",
    "    num_requests += 1\n",
    "    print(f\"\\r{num_requests} submitted | {num_responses} responses\", end=\"\", flush=True)\n",
    "    response = session.request(**request_params)\n",
    "    num_responses += 1\n",
    "    return response\n",
    "\n",
    "\n",
    "def chunk_list(lst, chunk_size):\n",
    "    \"\"\"Yield successive chunks of chunk_size from lst.\"\"\"\n",
    "    for i in range(0, len(lst), chunk_size):\n",
    "        yield lst[i : i + chunk_size]\n",
    "        \n",
    "\n",
    "def chunk_generator(generator, chunk_size):\n",
    "    \"\"\"Yield successive chunks of chunk_size from generator.\"\"\"\n",
    "    while True:\n",
    "        chunk = list(itertools.islice(generator, chunk_size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        yield chunk\n",
    "\n",
    "\n",
    "def fetch_responses(\n",
    "    requests_generator: Generator[Dict[str, Any], None, None], pool_size: int = 5\n",
    ") -> Generator[requests.Response, None, None]:\n",
    "    \"\"\"Function to manage a pool of HTTP workers\"\"\"\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=pool_size) as executor:\n",
    "        # Chunk the requests into groups of (pool_size * 10)\n",
    "        for chunk in chunk_generator(requests_generator, pool_size * 20):\n",
    "            futures = [executor.submit(http_worker, params) for params in chunk]\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                yield future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_apis = [\"https://honeyberry-spinach-04iea1s0vf4y7jef.salad.cloud\", \"https://kumquat-potato-hqka58l4iluyvpin.salad.cloud\"]\n",
    "\n",
    "\n",
    "def all_urls():\n",
    "    for url in list_all_file_urls(image_bucket, \"wikipedia/english 1/\"):\n",
    "        if url.endswith(\".ogg\"):\n",
    "            yield url\n",
    "    for url in list_all_file_urls(image_bucket, \"wikipedia/english 2/\"):\n",
    "        if url.endswith(\".ogg\"):\n",
    "            yield url\n",
    "    for url in list_all_file_urls(image_bucket, \"cv-corpus-15.0-2023-09-08/en/clips/\"):\n",
    "        if url.endswith(\".mp3\"):\n",
    "            yield url\n",
    "\n",
    "def all_requests(urls):\n",
    "    for url in urls:\n",
    "        for asr_api in asr_apis:\n",
    "            yield {\"method\": \"POST\", \"url\": f\"{asr_api}/asr\", \"json\": {\"url\": url}}\n",
    "\n",
    "def get_audio_url_from_response(response: requests.Response) -> str:\n",
    "    request_body = response.request.body\n",
    "    decoded_body = request_body.decode(\"utf-8\")\n",
    "    return json.loads(decoded_body)[\"url\"]\n",
    "\n",
    "\n",
    "def get_rows(request_generator):\n",
    "    for response in fetch_responses(request_generator, pool_size=5):\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Error: {response.status_code} {response.reason}\")\n",
    "        body = {}\n",
    "        body[\"audio_url\"] = get_audio_url_from_response(response)\n",
    "        salad_headers = [\n",
    "            header\n",
    "            for header in response.headers\n",
    "            if header\n",
    "            in [\n",
    "                \"x-gpu-name\",\n",
    "                \"x-salad-machine-id\",\n",
    "                \"x-salad-container-group-id\",\n",
    "                \"x-processing-time\",\n",
    "                \"x-audio-length\",\n",
    "                \"x-realtime-factor\",\n",
    "                \"x-model-id\",\n",
    "            ]\n",
    "        ]\n",
    "        for header in salad_headers:\n",
    "            body[header] = response.headers[header]\n",
    "        yield body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Audio Clips: 1159240\n"
     ]
    }
   ],
   "source": [
    "all_urls_file = \"all_urls.txt\"\n",
    "if not os.path.exists(all_urls_file):\n",
    "    with open(all_urls_file, \"w\") as f:\n",
    "        for url in all_urls():\n",
    "            f.write(f\"{url}\\n\")\n",
    "            \n",
    "with open(all_urls_file, \"r\") as f:\n",
    "    all_urls = f.readlines()\n",
    "    \n",
    "print(f\"Total Number of Audio Clips: {len(all_urls)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_requests_file = \"all_requests.jsonl\"\n",
    "if not os.path.exists(all_requests_file):\n",
    "    with open(all_requests_file, \"w\") as f:\n",
    "        for request in all_requests(all_urls):\n",
    "            f.write(f\"{json.dumps(request)}\\n\")\n",
    "\n",
    "def request_generator():\n",
    "    with open(all_requests_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row_file = \"all_rows.jsonl\"\n",
    "# if not os.path.exists(row_file):\n",
    "#     with open(row_file, \"w\") as f:\n",
    "#         for row in get_rows(request_generator()):\n",
    "#             f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "# with open(row_file) as f:\n",
    "#     rows = [json.loads(line) for line in f]\n",
    "\n",
    "# print(f\"Total number of rows: {len(rows)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
